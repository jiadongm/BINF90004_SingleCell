---
title: 'BINF90004 Single cell RNA-seq case study'
subtitle: 'Week1 - Overview & motivating examples'
author: 'Dr Jiadong Mao'
date: '2021'
fontsize: 10pt
output:
  beamer_presentation: 
    colortheme: rose
    latex_engine: xelatex
    includes:
      in_header: preamble.tex
    fonttheme: structurebold
---

## Welcome!
<!-- ## \red{Notation} -->

<!-- - $\p$: probability -->
<!-- - $p$: pmf pdf, never prob -->
<!-- - $\theta$: parameter, may be vector, eg $\theta=(\mu,\sigma^2)$ -->
<!-- - $\sample = (Y_1,\dots,Y_n)$, sample of size $n$ viewed as rvs -->
<!-- - $Y$: a generic $Y_i$ (iid as $\sample$) -->
<!-- - $\bmy = \{y_1,\dots,y_n\}$, sample of size $n$, observed (nonrandom) -->
<!-- - $y$: a generic $y_i$ -->
- About me.

- About you.


## Course information
- Lectures
    - 1.5h on Mon & Wed; 1h on Tue & Thur;
    - Lab session will be incorporated into the lectures.

- Consultation hours
    - Your preference?

- Q&A out of consultation hours: Ed.

- 3 quizzes, 5 assignments: goal is to monitor learning, will involve R programming (no more than what appeared in the lectures).

- Mid-term exam, final exam: mainly about understanding, some derivagions, no programming.

- Any question?




## Course overview
```{r, echo=T}
knitr::opts_chunk$set(cache=T, message = F, echo=T)
```
- What the course is about?
  - Bayesian inference: a Bayesian version of undergraduate level mathematical statistics;
  - Topics include: fitting distributions to data, group comparisons, linear models, generalised linear models;
  - Learn to appreciate advantages of Bayesian approaches.
  
- Mathematics: will involve a lot of derivations; but much has been seen in prerequisite courses.

- R: basic R programming; more advanced content appears but won't be examined.

- Resources for maths:
    - Focus on slides;
    - Read relevant textbook chapters if you need more information.
    
- The Textbooks: Hoff (2009), Gelman et al. (2014). 

- Resources for R: see Sakai page.

## Acknowldegement
Slides used in STATS 304 are written by Dr Jiadong Mao, based on Hoff (2009), Gelman et al. (2014) and the slides for the course STA 360 at Duke University by Dr Olanrewaju Michael Akande and Dr Alexander Volfovsky.


## Background: Elementary Bayes' rule
\begin{block}{Bayes' formula}
		\begin{equation*}
			\red{\p(A\mid B)} = \frac{\p(B\mid A) \fg{\p(A)}}{\p(B\mid A)\fg{\p(A)} +             \p(B\mid A^c)\fg{\p(A^c)}}.
		\end{equation*}
	Interpretation: \fg{prior belief/knowledge} of $A$ to \red{updated                 belief/knowledge} of $A$, due to extra info/conditioning.
\end{block}

- Example (testing drug use):
  - Illegal drug $D$ and test $T$, want to know positive predicative value                $\p(D\mid T)$;
  - Suppose prevalence $\p(D) = 0.05$, test sensitivity (true positive)                  $\p(T\mid D) = 0.9$, specificity (true negative) $\p(T^c \mid D^c) = 0.95$;
  - PPV: \tiny
		$$\p(D\mid T) = \frac{\p(T\mid D)\p(D)}{\p(T\mid D)\p(D) + \p(T\mid D^c)\p(D^c)} =       \frac{0.9\times 0.05}{0.9\times 0.05 + 0.05\times 0.95} = 0.4865 $$
		
- Interpretation.



## Basic ingredients of Bayesian inference

- In real life, we collect data to update our belief: 
  - *Data set* $\bmy = \{y_1,\dots,y_n\}$: dataset with *sample size* n;
  - Data drawn from *random variable* $Y \sim p(y)$, $y\in\Y$;
  - Often assume $p(y)$ is from a *parametric family*: $p(y) = p(y\mid \theta)$, with *parameter* $\theta \in \Theta$, eg $Y\sim N(\mu,\sigma^2)$, $\theta=(\mu,\sigma^2)$
  - \red{View $\theta$ as random as well.} $\theta\sim p(\theta)$, the *prior  distribution*.
  - Call $\Y$ *sample space* and $\Theta$ *parameter space*;
	
- Interpretation:
  - $p(\theta)$ as a fun over $\Theta$ describes our \fg{prior belief};
  - $p(y\mid \theta)$ is the (parametric) *sampling model*, representing what we know from data;
  - Eg $Y\sim N(\mu,\sigma^2)$, $\Y=\R$, $\theta=(\mu,\sigma^2)$, $\Theta=\R\times\R_+$;
  - Goal is to update our belief about $\theta$, ie obtain *posterior distribution* $p(\theta\mid y)$.
  

## Bayes rule: more generalised version
\begin{block}{Bayes' rule}
	$$
			\red{p(\theta\mid y)} = \frac{p(y\mid \theta) \fg{p(\theta)}}{\int_{\Theta} p(y\mid \vartheta) \fg{p(\vartheta)} \dif \vartheta }
	$$
$$
	\red{\p(A\mid B)} = \frac{\p(B\mid A) \fg{\p(A)}}{\p(B\mid A)\fg{\p(A)} + \p(B\mid A^c)\fg{\p(A^c)}}.
$$
\end{block}

- If $p(\theta)$ and $p(y\mid \theta)$ represent a rational person's beliefs, then Bayes' rule is an `optimal' method of updating this person's beliefs about $\theta$ given information about $y$. Cox (1946, 1961); Savage (1954, 1972).







## Frequentist vs Bayesian
- Choice of $p(\theta)$ too `subjective'? Frequentists only use $p(y\mid \theta)$ (method of moments, MLE etc).

- 'Uninformative' prior.

- A very general comparison:
    - Bayesians choose prior $p(\theta)$; frequentists choose method/algorithm $t(\bmy)$;
    - Bayesians answer \red{all} questions about $\theta$; frequentists answer question at hand;
    - Modern complex data applications tend to use or even combine both, eg empirical Bayes.


## Reading list

Suggested:

- Efron, B., 1986. Why isn't everyone a Bayesian?. The American
Statistician, 40(1), pp. 1-5.
- Gelman, A., 2008. Objections to Bayesian statistics. Bayesian Analysis,
3(3), pp. 445-449.
- Dunson, D. B., 2018. Statistics in the big data era: Failures of the
machine. Statistics & Probability Letters, 136, pp. 4-9.

More technical:

- Diaconis, P., 1977. Finite forms of de Finetti's theorem on
exchangeability. Synthese, 36(2), pp. 271-281.
- Gelman, A., Meng, X. L. and Stern, H., 1996. Posterior predictive
assessment of model fitness via realized discrepancies. Statistica sinica,
pp. 733-760.

Further:

- Fiducial inference
- Empirical Bayes




## Priors are flexible 
- Objections: priors
- All models are wrong. It's \fg{how data change our beliefs} that's important.




## Example: est prob of rare events
- Interested in est \fg{prevalence} $\theta$ of disease.
- Random sample of size 20 collected.
- Parameter space: $\Theta$, sample space $\mathcal{Y}=\{0,1,\dots,20\}$.
- Sampling model:
$$
 Y\mid \theta \sim \bin(20,\theta)
$$
- Prior?
  - Must be distr on $\Theta=[0,1]$
  - Eg ${\rm Unif}(0,1)$, but need more flexibility
- Beta distr with parameters $(a,b)\in\R_+\times \R_+$:
$$
\pi(\theta)=\frac{1}{B(a, b)} \theta^{a-1}(1-\theta)^{b-1},
$$
where $B$ is beta function (normalising const).
- $\E(\theta) = a/(a+b)$ and $\var(\theta) = ab/\{(a+b)^2 (a+b+1) \}$: 
  - $w=a+b$ is 'concentration parameter'
  - note $\sigma^2_{\theta} = (1-\mu_{\theta})\mu_{\theta}/(w+1)$



## Example (contd)
```{r, echo=F, message=F, fig.dim=c(10,7), cache=T}
library(tidyverse)
plot_beta <- tibble(theta=seq(0,1,length.out=200),
                    'beta(1,1)'=dbeta(theta,1,1),
                    'beta(0.5,0.5)'=dbeta(theta,0.5,0.5),
                    'beta(2,2)'=dbeta(theta,2,2),
                    'beta(1,5)'=dbeta(theta,1,5),
                    'beta(5,1)'=dbeta(theta,5,1),
                    'beta(2,5)'=dbeta(theta,2,5))
plot_beta %>% 
  pivot_longer(cols=starts_with('beta')) %>% 
  ggplot(aes(x=theta,y=value)) + 
  geom_line(aes(colour=name), size=1)
```




## Posterior
- With prior $\theta \sim \betad(a,b)$, sampling model $Y\mid\theta \sim \bin(\theta)$, by Bayes' rule:
$$
\theta\mid {Y=y} \sim \betad(a+y, b+n-y)
$$
- \fg{How data change belief}: eg $a=2, b=20, y=0$
```{r, echo=F, fig.dim=c(8,5), cache=T}
plot_beta <- tibble(theta=seq(0,1,length.out=200),
                    beta1=dbeta(theta,2,20),
                    beta2=dbeta(theta,2,40))
plot_beta %>% 
  pivot_longer(cols=starts_with('beta')) %>% 
  ggplot(aes(x=theta,y=value)) + 
  geom_line(aes(colour=name)) +
  xlab(TeX('$\\theta$')) +
  ylab(TeX('$\\pi(\\theta)$'))
```